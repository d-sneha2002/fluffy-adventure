{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.**\n",
    "\n",
    "Eigenvalues: Eigenvalues are scalar values associated with a linear transformation representing how a vector is stretched or shrunk along its direction when the transformation is applied.   \n",
    "Eigenvectors: Eigenvectors are non-zero vectors that, when multiplied by a matrix, result in a scalar multiple of themselves. In other words, they maintain their direction but may change in magnitude.   \n",
    "Relation to Eigen-Decomposition: Eigen-Decomposition is a process of factoring a square matrix into a product of three matrices: a matrix of eigenvectors, a diagonal matrix of eigenvalues, and the inverse of the eigenvector matrix.\n",
    "Example:\n",
    "\n",
    "Consider the matrix A = [[2, 1], [1, 2]]. Let's find its eigenvalues and eigenvectors.\n",
    "\n",
    "Characteristic Equation: Calculate the determinant of (A - λI), where λ is an eigenvalue and I is the identity matrix.\n",
    "\n",
    "det(A - λI) = det([[2-λ, 1], [1, 2-λ]]) = (2-λ)^2 - 1 = λ^2 - 4λ + 3 = 0\n",
    "Solve for Eigenvalues: Solving the characteristic equation gives us λ1 = 1 and λ2 = 3.\n",
    "\n",
    "Find Eigenvectors: For each eigenvalue, solve the equation (A - λI)v = 0 to find the corresponding eigenvector v.\n",
    "\n",
    "For λ1 = 1:\n",
    "(A - I)v1 = [[1, 1], [1, 1]]v1 = 0\n",
    "Solution: v1 = [1, -1]\n",
    "For λ2 = 3:\n",
    "(A - 3I)v2 = [[-1, 1], [1, -1]]v2 = 0\n",
    "Solution: v2 = [1, 1]\n",
    "\n",
    "**Q2. What is eigen decomposition and what is its significance in linear algebra?**\n",
    "\n",
    "Eigen-decomposition is the process of factoring a square matrix into a product of three matrices: a matrix of eigenvectors, a diagonal matrix of eigenvalues, and the inverse of the eigenvector matrix. It's significant because it allows us to analyze and understand the behavior of linear transformations in terms of their eigenvectors and eigenvalues.\n",
    "\n",
    "**Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.**\n",
    "\n",
    "A square matrix is diagonalizable if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix. This means the eigenvectors form a basis for the vector space.   \n",
    "\n",
    "Proof:\n",
    "\n",
    "If a matrix A has n linearly independent eigenvectors v1, v2, ..., vn with corresponding eigenvalues λ1, λ2, ..., λn, we can form the matrix P = [v1, v2, ..., vn]. Then, AP = PΛ, where Λ is the diagonal matrix with eigenvalues on the diagonal. Since P is invertible (because its columns are linearly independent), we can write A = PΛP^-1, which is the eigen-decomposition form.\n",
    "\n",
    "**Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.**  \n",
    "\n",
    "The spectral theorem states that a symmetric matrix can be diagonalized by an orthogonal matrix. In other words, its eigenvectors are orthogonal, and the diagonal matrix Λ contains its eigenvalues. This is significant because it allows us to represent symmetric matrices in a simpler form, making calculations and analysis easier.   \n",
    "\n",
    "Example:\n",
    "\n",
    "The matrix A = [[2, 1], [1, 2]] from Q1 is symmetric. Its eigenvectors, [1, -1] and [1, 1], are orthogonal. Therefore, A can be diagonalized using the spectral theorem.\n",
    "\n",
    "**Q5. How do you find the eigenvalues of a matrix and what do they represent?**\n",
    "\n",
    "To find eigenvalues, we solve the characteristic equation det(A - λI) = 0. The eigenvalues represent the scaling factors associated with the corresponding eigenvectors under the linear transformation represented by the matrix.\n",
    "\n",
    "**Q6. What are eigenvectors and how are they related to eigenvalues?**\n",
    "\n",
    "Eigenvectors are non-zero vectors that, when multiplied by a matrix, result in a scalar multiple of themselves. The scalar multiple is the corresponding eigenvalue.   \n",
    "\n",
    "**Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?**\n",
    "\n",
    "Eigenvectors represent the directions in which a linear transformation acts by scaling. Eigenvalues represent the scaling factors along these directions.\n",
    "\n",
    "**Q8. What are some real-world applications of eigen decomposition?**\n",
    "\n",
    "Eigen-decomposition has applications in:\n",
    "\n",
    "Image and signal processing: Compression, denoising, feature extraction.\n",
    "Data mining and machine learning: Principal Component Analysis (PCA), dimensionality reduction, clustering.\n",
    "Physics and engineering: Solving differential equations, analyzing vibrations, studying quantum mechanics.\n",
    "\n",
    "**Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?**\n",
    "\n",
    "Yes, a matrix can have multiple sets of eigenvectors and eigenvalues if it has repeated eigenvalues. In this case, the eigenvectors corresponding to the repeated eigenvalue will form a subspace.   \n",
    "\n",
    "**Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.**  \n",
    "\n",
    "Eigen-decomposition is valuable in data analysis and machine learning due to its ability to simplify complex data and extract meaningful information.   \n",
    "\n",
    "Principal Component Analysis (PCA): Reduces dimensionality by finding the directions of maximum variance in the data (eigenvectors) and projecting the data onto these directions.\n",
    "Singular Value Decomposition (SVD): Factorizes a matrix into three matrices, similar to eigen-decomposition but applicable to non-square matrices. Used in image compression, recommendation systems, and collaborative filtering.\n",
    "Markov Chains: Analyzing the long-term behavior of systems with multiple states using the eigenvalues and eigenvectors of the transition matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
