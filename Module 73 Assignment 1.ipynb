{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the KNN algorithm?**\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is based on the principle that similar things exist in close proximity. In KNN, a data point is classified or predicted based on the majority of its K nearest neighbors.   \n",
    "\n",
    "**Q2. How do you choose the value of K in KNN?**\n",
    "\n",
    "The value of K in KNN represents the number of neighbors considered when making a prediction. Choosing the right value of K is crucial for the performance of the algorithm. There are several methods to choose the value of K, including:\n",
    "- Cross-validation: This involves training the model with different values of K and selecting the value that gives the best performance on a validation set.\n",
    "- Elbow method: This method involves plotting the performance of the model for different values of K and choosing the value where the performance starts to plateau.\n",
    "- Domain knowledge: If you have prior knowledge about the data, you can use this knowledge to select a suitable value of K.\n",
    "\n",
    "**Q3. What is the difference between KNN classifier and KNN regressor?**\n",
    "\n",
    "The main difference between KNN classifier and KNN regressor is the type of output they produce:   \n",
    "- KNN classifier: Predicts the class label of a data point based on the majority class of its K nearest neighbors.   \n",
    "- KNN regressor: Predicts the numerical value of a data point based on the average value of its K nearest neighbors.\n",
    "\n",
    "**Q4. How do you measure the performance of KNN?**\n",
    "\n",
    "The performance of KNN can be measured using various metrics, depending on the type of problem:\n",
    "- Classification: Accuracy, precision, recall, F1-score, confusion matrix, ROC curve, AUC\n",
    "- Regression: Mean squared error (MSE), mean absolute error (MAE), R-squared\n",
    "\n",
    "**Q5. What is the curse of dimensionality in KNN?**\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN degrades as the number of features increases. This is because as the dimensionality increases, the distance between data points becomes less meaningful, and it becomes harder to find close neighbors.   \n",
    "\n",
    "**Q6. How do you handle missing values in KNN?**\n",
    "\n",
    "There are several ways to handle missing values in KNN:\n",
    "- Imputation: Fill in missing values with mean, median, or mode of the corresponding feature.\n",
    "- Deletion: Remove data points with missing values.\n",
    "- Distance-based imputation: Use the values of the K nearest neighbors to estimate the missing value.\n",
    "\n",
    "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "\n",
    "KNN classifier is generally better for classification problems, while KNN regressor is better for regression problems. However, the performance of both algorithms depends on the specific dataset and problem.\n",
    "\n",
    "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "\n",
    "Strengths of KNN:   \n",
    "- Simple to understand and implement.\n",
    "- No training phase required.\n",
    "- Effective for small datasets with clear boundaries between classes.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "- Can be computationally expensive for large datasets.\n",
    "- Sensitive to the choice of distance metric.\n",
    "- Prone to overfitting if K is too small.\n",
    "\n",
    "To address these weaknesses,\n",
    "\n",
    "- Use dimensionality reduction techniques to reduce the number of features.\n",
    "- Experiment with different distance metrics.\n",
    "- Use cross-validation to find the optimal value of K.\n",
    "\n",
    "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "\n",
    "Euclidean distance and Manhattan distance are two common distance metrics used in KNN:\n",
    "- Euclidean distance: Calculates the straight-line distance between two points.\n",
    "- Manhattan distance: Calculates the sum of the absolute differences between corresponding coordinates of two points.\n",
    "\n",
    "The choice of distance metric can affect the performance of KNN.\n",
    "\n",
    "**Q10. What is the role of feature scaling in KNN?**\n",
    "\n",
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculation. Without feature scaling, features with larger ranges can dominate the distance calculation, leading to inaccurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
