{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?**\n",
    "\n",
    "- Overfitting: This occurs when a model learns the training data too well, capturing the noise and random fluctuations in the data. This results in a model that performs poorly on new, unseen data.\n",
    "- Underfitting: This happens when a model is too simple to capture the underlying patterns in the data. It results in a model that performs poorly on both training and new data.\n",
    "\n",
    "Consequences of overfitting:\n",
    "- High training accuracy but low test accuracy.\n",
    "- Poor generalization to new data.\n",
    "- Model is too complex and sensitive to noise in the data.\n",
    "\n",
    "Consequences of underfitting:\n",
    "- Low training and test accuracy.\n",
    "- Model is too simple to capture the complexity of the data.\n",
    "\n",
    "Mitigating overfitting:\n",
    "- Collect more data.\n",
    "- Use regularization techniques (L1, L2, dropout).\n",
    "- Simplify the model.\n",
    "- Cross-validation.\n",
    "\n",
    "Mitigating underfitting:\n",
    "- Add more features to the data.\n",
    "- Use a more complex model.\n",
    "- Increase the number of parameters in the model.\n",
    "\n",
    "**Q2: How can we reduce overfitting? Explain in brief.**\n",
    "\n",
    "Overfitting can be reduced by using techniques such as regularization, cross-validation, and increasing the size of the training dataset.\n",
    "\n",
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. This can happen when:   \n",
    "- The model is too simple for the complexity of the data.   \n",
    "- The training data is insufficient.\n",
    "- The model is not trained for a long enough time.\n",
    "\n",
    "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?**\n",
    "\n",
    "The bias-variance tradeoff is the relationship between the complexity of a model and its ability to generalize to new data.\n",
    "\n",
    "- Bias: This is the error due to the model's assumptions about the data. A high-bias model is too simple and will underfit the data.\n",
    "- Variance: This is the error due to the model's sensitivity to fluctuations in the training data. A high-variance model is too complex and will overfit the data.\n",
    "\n",
    "The goal is to find a model with a balance between bias and variance, resulting in good generalization performance.\n",
    "\n",
    "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?**\n",
    "\n",
    "Overfitting and underfitting can be detected by:\n",
    "\n",
    "- Learning curves: Plot the training and validation error as a function of the training set size. If the training error is much lower than the validation error, the model is likely overfitting. If both errors are high, the model is likely underfitting.\n",
    "- Cross-validation: Evaluate the model's performance on multiple subsets of the data. If the performance on the validation sets is significantly worse than on the training set, the model is likely overfitting.\n",
    "- Complexity analysis: Compare the performance of models with different levels of complexity. If a simpler model performs similarly to a more complex model, the complex model is likely overfitting.\n",
    "\n",
    "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?**\n",
    "\n",
    "- Bias: High bias models are too simple and underfit the data. Examples include linear regression and Naive Bayes.   \n",
    "- Variance: High variance models are too complex and overfit the data. Examples include decision trees and neural networks.\n",
    "\n",
    "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.**\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the model's loss function. This encourages the model to be simpler and less complex.   \n",
    "\n",
    "- L1 regularization: Adds the sum of the absolute values of the model's coefficients to the loss function. This can lead to sparse models where some coefficients are zero.\n",
    "- L2 regularization: Adds the sum of the squares of the model's coefficients to the loss function. This encourages smaller weights and reduces the impact of outliers.\n",
    "- Dropout: Randomly drops units (neurons) during training, preventing the model from relying too much on any one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
