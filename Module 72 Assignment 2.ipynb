{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Gradient Boosting Regression?**\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that uses an ensemble of decision trees to make predictions. It is a supervised learning algorithm used for regression tasks, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_boosting(X, y, n_estimators, learning_rate, max_depth):\n",
    "    \"\"\"\n",
    "    Implements a simple gradient boosting algorithm from scratch.\n",
    "\n",
    "    Args:\n",
    "        X: The feature matrix.\n",
    "        y: The target variable.\n",
    "        n_estimators: The number of trees in the ensemble.\n",
    "        learning_rate: The learning rate.\n",
    "        max_depth: The maximum depth of each tree.\n",
    "\n",
    "    Returns:\n",
    "        The predicted values.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    y_pred = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        residual = y - y_pred\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(X, residual)\n",
    "        y_pred += learning_rate * tree.predict(X)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.**\n",
    "\n",
    "Here is an example of how to use grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. What is a weak learner in Gradient Boosting?**\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple model that is only slightly better than random guessing. It is typically a decision tree with a small depth.\n",
    "\n",
    "**Q5. What is the intuition behind the Gradient Boosting algorithm?**\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictions of a model by adding new weak learners that focus on the errors of the previous models.\n",
    "\n",
    "**Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?**\n",
    "\n",
    "Gradient Boosting algorithm builds an ensemble of weak learners by sequentially adding new weak learners to the ensemble. Each new weak learner is trained to predict the residuals of the previous ensemble.\n",
    "\n",
    "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**\n",
    "\n",
    "The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are:\n",
    "- Define the loss function.\n",
    "- Calculate the gradient of the loss function with respect to the predictions.\n",
    "- Fit a weak learner to the negative gradient.\n",
    "- Update the predictions by adding a scaled version of the weak learner's predictions.\n",
    "- Repeat steps 3 and 4 for a fixed number of iterations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
