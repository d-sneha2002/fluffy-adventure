{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Random Forest Regressor?**\n",
    "\n",
    "A Random Forest Regressor is a machine learning algorithm that uses an ensemble of decision trees to make predictions. It is a supervised learning algorithm used for regression tasks, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "**Q2. How does Random Forest Regressor reduce the risk of overfitting?**\n",
    "\n",
    "Random Forest Regressor reduces the risk of overfitting through the following mechanisms:   \n",
    "\n",
    "Bagging: It uses bagging (Bootstrap Aggregating), where each decision tree is trained on a random subset of the data with replacement. This helps to reduce the variance of individual trees.\n",
    "Random feature selection: At each node of a decision tree, only a random subset of features is considered for splitting. This further decorrelates the trees and reduces overfitting.\n",
    "\n",
    "**Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?**\n",
    "\n",
    "Random Forest Regressor aggregates the predictions of multiple decision trees by averaging their individual predictions. Each tree in the forest provides an estimate for the target variable, and the final prediction is obtained by taking the average of these estimates.   \n",
    "\n",
    "**Q4. What are the hyperparameters of Random Forest Regressor?**\n",
    "\n",
    "Some common hyperparameters of Random Forest Regressor include:\n",
    "\n",
    "- n_estimators: The number of decision trees in the forest.\n",
    "- max_depth: The maximum depth of each decision tree.\n",
    "- min_samples_split: The minimum number of samples required to split an internal node.   \n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "- max_features: The number of features to consider when looking for the best split at each node.   \n",
    "\n",
    "**Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?**\n",
    "\n",
    "The main difference between Random Forest Regressor and Decision Tree Regressor is that Random Forest Regressor is an ensemble method that combines multiple decision trees, while Decision Tree Regressor uses a single decision tree. Random Forest Regressor is generally more robust and less prone to overfitting than Decision Tree Regressor.   \n",
    "\n",
    "**Q6. What are the advantages and disadvantages of Random Forest Regressor?**\n",
    "\n",
    "Advantages:\n",
    "- Handles both numerical and categorical data.\n",
    "- Effective for both regression and classification tasks.\n",
    "- Less prone to overfitting compared to single decision trees.\n",
    "- Provides feature importance scores.\n",
    "- Can handle missing values.\n",
    "\n",
    "Disadvantages:\n",
    "- Can be computationally expensive for large datasets.\n",
    "- Less interpretable than single decision trees.\n",
    "\n",
    "**Q7. What is the output of Random Forest Regressor?**\n",
    "\n",
    "The output of Random Forest Regressor is a continuous numerical value, which is the predicted value for the target variable.\n",
    "\n",
    "**Q8. Can Random Forest Regressor be used for classification tasks?**\n",
    "\n",
    "Yes, Random Forest Regressor can be used for classification tasks by modifying the output to predict class probabilities instead of continuous values. There are specific implementations of Random Forest for classification, but the underlying principles remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
