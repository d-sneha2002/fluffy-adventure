{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?**\n",
    "\n",
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way they calculate the distance between two points. Euclidean distance calculates the straight-line distance between two points, while Manhattan distance calculates the sum of the absolute differences between corresponding coordinates of two points.   \n",
    "\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor. Euclidean distance is often preferred when the features are continuous and have similar scales. Manhattan distance can be more appropriate when the features are discrete or have different scales.   \n",
    "\n",
    "**Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?**\n",
    "\n",
    "The optimal value of k for a KNN classifier or regressor depends on the specific dataset and problem. Several techniques can be used to determine the optimal k value, including:\n",
    "\n",
    "- Cross-validation: This involves training the model with different values of k and selecting the value that gives the best performance on a validation set.\n",
    "- Elbow method: This method involves plotting the performance of the model for different values of k and choosing the value where the performance starts to plateau.\n",
    "- Domain knowledge: If you have prior knowledge about the data, you can use this knowledge to select a suitable value of k.\n",
    "\n",
    "**Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?**\n",
    "\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor. Euclidean distance is often preferred when the features are continuous and have similar scales. Manhattan distance can be more appropriate when the features are discrete or have different scales.\n",
    "\n",
    "**Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?**\n",
    "\n",
    "Some common hyperparameters in KNN classifiers and regressors include:\n",
    "\n",
    "- k: The number of neighbors considered when making a prediction.\n",
    "- Distance metric: The method used to calculate the distance between data points.\n",
    "- Weighting function: The way the contributions of neighbors are weighted.\n",
    "- Algorithm: The algorithm used to find the nearest neighbors.\n",
    "\n",
    "These hyperparameters can affect the performance of the model. To tune these hyperparameters, you can use techniques like cross-validation and grid search.\n",
    "\n",
    "**Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?**\n",
    "\n",
    "The size of the training set can affect the performance of a KNN classifier or regressor. A larger training set can improve the accuracy of the model, but it can also increase the computational cost. To optimize the size of the training set, you can use techniques like undersampling and oversampling.\n",
    "\n",
    "**Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?**\n",
    "\n",
    "Some potential drawbacks of using KNN as a classifier or regressor include:\n",
    "\n",
    "- Computational cost: KNN can be computationally expensive for large datasets.   \n",
    "- Sensitivity to noise: KNN can be sensitive to noise in the data.\n",
    "- Curse of dimensionality: KNN can suffer from the curse of dimensionality, where the performance of the model degrades as the number of features increases.\n",
    "\n",
    "To overcome these drawbacks, we use techniques like dimensionality reduction, feature selection, and distance weighting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
